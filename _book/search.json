[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Preface\nThis is a Quarto book containing my notes as I work through McElreath (2020), using also the accompanying YouTube lecture videos (2023), and Kurz (2023). I’m working through the book as part of a bi-weekly meeting with members of the ZAS Berlin, with shared materials available on GitHub.\n\n\n\n\n\n\nKurz, A. Solomon. 2023. Statistical Rethinking with brms, ggplot2, and the tidyverse. Version 1.3.0. https://bookdown.org/content/3890/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press."
  },
  {
    "objectID": "setup.html#packages",
    "href": "setup.html#packages",
    "title": "Set-up",
    "section": "Packages",
    "text": "Packages\nTo be run prior to the other scripts.\n\ninstall.packages(\"pacman\")\npacman::p_load(devtools)\n\n# install developer packages\nremotes::install_github(\"bnicenboim/bcogsci\")\nremotes::install_github(\"stan-dev/cmdstanr\")\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\",\"shape\"))\ndevtools::install_github(\"rmcelreath/rethinking\")\n\n\npacman::p_load(\nMASS,\n## be careful to load dplyr after MASS\ndplyr,\ntidyr,\npurrr,\nextraDistr,\nggplot2,\nloo,\nbridgesampling,\nbrms,\nbayesplot,\ntictoc,\nhypr,\nbcogsci,\nlme4,\nrethinking,\nrstan,\n# This package is optional, see https://mc-stan.org/cmdstanr/:\ncmdstanr,\n# This package is optional, see https://hyunjimoon.github.io/SBC/:\nSBC, \nSHELF,\nrootSolve\n)\n\n# could be added to the top of scripts:\n## Save compiled models:\nrstan_options(auto_write = FALSE)\n## Parallelize the chains using all the cores:\noptions(mc.cores = parallel::detectCores())\n# To solve some conflicts between packages:\nselect &lt;- dplyr::select\nextract &lt;- rstan::extract"
  },
  {
    "objectID": "setup.html#writing-tips",
    "href": "setup.html#writing-tips",
    "title": "Set-up",
    "section": "Writing tips",
    "text": "Writing tips\nTo render documents with equations in HTML and PDF, specifically with alignment and numbering of multiple equations in one chunk, avoid using $$ and \\begin{align}. I like $$ because it gives you a preview of your equation, but it crashes when rendering a PDF with \\begin{align}, because pandoc reads this as doubly opening an equation/mathmode (as far as I understand it). So, the solution is to just use \\begin{align} when I want to align more than one equation.\nThe following will not allow PDF to render:\n\n# will render in HTML but not PDF:\n$$\n\\begin{align}\ny &\\sim Normal(\\mu,\\sigma) \\label{eq-normal} \\\\\ny &\\sim binomial(N = 1, p) \\label{eq-binomial}\n\\end{align}\n$$\n\nThe following will allow PDF to render, and will be printed as in ?eq-normal1 and ?eq-binomial2\n\n# will render in both HTML and PDF:\n\\begin{align}\ny &\\sim Normal(\\mu,\\sigma) \\label{eq-normal} \\\\\ny &\\sim binomial(N = 1, p) \\label{eq-binomial}\n\\end{align}\n\n\\[\\begin{align}\ny &\\sim Normal(\\mu,\\sigma) \\label{eq-normal1} \\\\\ny &\\sim binomial(N = 1, p) \\label{eq-binomial2}\n\\end{align}\\]\nIt’s still helpful to use $$ when writing so you can check that your equation will be printed as you intend. Just remember to remove them/comment them out before rendering."
  },
  {
    "objectID": "notes/02-small_worlds.html#terms-and-concepts",
    "href": "notes/02-small_worlds.html#terms-and-concepts",
    "title": "1  Small Worlds and Large Worlds",
    "section": "1.1 Terms and Concepts",
    "text": "1.1 Terms and Concepts\n\n\n\n\n\n\n  \n    \n    \n      term\n      definition\n    \n  \n  \n    conjecture\n\npossible outcomes (like the 5 possible proportions of blue/white marbles in the bag)\n\n    plausibility\n\nthings that can happen more than one way are plausible; we want to find out which combinations of blue/white marbles is most plausible\n\n    likelihood (general)\n\nrelative number of ways that a value p can produce the data; derived by enumerating all possible data sequences that could have happened, and eliminating those inconsistent with the data\n\n    likelihood (Bayesian)\n\ndistributions of variables; the observed data\n\n    prior probability\n\nprior plausibility of any specific p\n\n    prior\n\ndistribution of prior plausibility\n\n    posterior probability\n\nnew, updated plausibility of any specific p given priors + data\n\n    Bayesian updating\n\nupdating prior plausibilties in light of the data to produce posterior plausibilties\n\n    variables\n\nsymbols that can tkae on a different value; e.g., the coutns of water and land in the small world tossing example\n\n    parameters\n\nunobserved variables; e.g., the proportion of water on the globe (in the land/water tossing example). We still have to define them, however (e.g., probabilities in a binomial distribution are not always known)\n\n    maximal entropy\n\nthe distrubtion contains no additional information other then: tehre are 2 events, and the probabilities of each in each trial are p and 1 - p\n\n    Bayes’ theorem\n\n$$Pr(p|W,L) = \\frac{Prob\\ of\\ data\\ x\\ Prior}{Average\\ Prob\\ of\\ data}$$\n\n    Grid approximation\n\ncompute the posterior probability for any and every particular value of a paramter (p’): multiply the prior probability of p’ by the likelihood at p’\n\n    quadratic approximation\n\na Guassian approximation is quadratic approximation because the log of the Guassian distribution forms a parabola, and a parabola is a quadratic function"
  },
  {
    "objectID": "notes/02-small_worlds.html#code-from-chapter",
    "href": "notes/02-small_worlds.html#code-from-chapter",
    "title": "1  Small Worlds and Large Worlds",
    "section": "1.2 Code from chapter",
    "text": "1.2 Code from chapter\n\n1.2.1 Plausibilties\nHow probable is each possible combination of white/blue marbles? (divide each # of ways each combination could produce the observed data by sum of all ways. Ways: all white = 0 ways, 1 blue = 3, 2 blue = 8, etc.)\n\n# R code 2.1: compute plausibilities\nways &lt;- c(0,3,8,9,0)\nways/sum(ways)\n\n[1] 0.00 0.15 0.40 0.45 0.00\n\n\nSo, given that we have taken 2 blue and 1 white out of the bag is 0, the plausibility of:\n\nall marbles being white (0 ways) = 0\n1 blue, 3 white (3 ways) = 0.5\n2 blue, 2 white (8 ways) = 0.4\n3 blue, 1 white (9 ways) = .5\n4 blue, 0 white = 0\n\n\n\n1.2.2 Binomial distribution (dbinom)\nIn the water/land world-tossing example, what is the lieklihood of the data (where we observed 6 water and 3 land) if we assume the probability of observing ‘water’ is 0.5?\n\n# R code 2.2: binomial distribution\ndbinom(6, size = 9, prob = .5)\n\n[1] 0.1640625\n\n\n\n\n1.2.3 Grid approximation\nBuild a grid approximation for the model we’ve built so far using the following steps:\n\nDefine the grid (decide how many points to use in estimateing the posterior, and make a list of the parameter value on the grid)\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the unstandardized posterior at each parameter value, by multiplying the prior by the likelihood.\nFinally, standardize the posterior, by dividing each value by the sum of all values.\n\nHere we will make a grid of just 20 points:\n\n# R code 2.4: grid approximation for 20 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=20 ) \n\n# 2) define prior \nprior &lt;- rep( 1 , 20 ) \n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_20 &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"20 points\")\n\n\n## Repeat for 200 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=200 ) \n\n# 2) define prior \nprior &lt;- rep( 1 , 200 ) \n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n## Repeat for 5 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=5 ) \n\n# 2) define prior \nprior &lt;- rep( 1 , 5 ) \n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\nfig_grid_5 + fig_grid_20 + fig_grid_200\n\n\n\n\n\n\n1.2.4 Adjust priors\n\n## Repeat for 5 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=200 ) \n\n# 2) define prior \nprior &lt;- ifelse( p_grid &lt; 0.5 , 0 , 1 ) \n# prior &lt;- exp( -5*abs( p_grid - 0.5 ) )\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# fig_grid_5_prior &lt;-\n  cbind(p_grid, posterior, prior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  geom_line(aes(y = prior), colour = \"blue\") +\n  theme_minimal() +\n  labs(title = \"200 points with prior\")\n\n\n\n\nOkay a bit wonky (different y-scales), but we get the point.\n\n\n1.2.5 Quadratic approximation\nWe use the quap function from the rethinking package. This function takes a formula which defines the probability of the data in the prior.\n\n# R code 2.6\nglobe.qa &lt;- quap( alist( W ~ dbinom( W+L ,p) , # binomial likelihood \n                         p ~ dunif(0,1) # uniform prior \n                         ), data=list(W=6,L=3) ) \n# display summary of quadratic approximation \nprecis( globe.qa ) \n\n       mean        sd      5.5%     94.5%\np 0.6666666 0.1571338 0.4155365 0.9177968\n\n\nPrint output with precis():\n\nmean: posterior mean value of p (peak of the curvature)\nsd: the curvature; standard deviation of the posterior distribution\n5.5%-94.5%: 89% percentile\n\nThis output can be read Assuming the posterior is Gaussian, it is maximized at 0.67, and its standard deviation is 0.16.\n\n\n1.2.6 Check curvature\n\n# analytical calculation \nW &lt;- 6 \nL &lt;- 3 \ncurve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 ) \n# quadratic approximation \ncurve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )\n\n\n\n\n\n\n1.2.7 Markov chain Monte Carlo\n\n# R code 2.8\n\nn_samples &lt;- 1000 \np &lt;- rep(NA , n_samples) \np[1] &lt;- 0.5 \nW &lt;- 6 \nL &lt;- 3 \n\nfor (i in 2:n_samples) {\n    p_new &lt;- rnorm(1 , p[i - 1] , 0.1) \n    if (p_new &lt; 0)  p_new &lt;- abs(p_new) \n    if (p_new &gt; 1) p_new &lt;- 2 - p_new \n    q0 &lt;- dbinom(W , W + L , p[i - 1]) \n    q1 &lt;- dbinom(W , W + L , p_new) \n    p[i] &lt;- ifelse(runif(1) &lt; q1 / q0 , p_new , p[i - 1])\n  }\n\n\n# R code 2.9\ndens( p , xlim=c(0,1) ) \ncurve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )"
  },
  {
    "objectID": "notes/02-small_worlds.html#practice",
    "href": "notes/02-small_worlds.html#practice",
    "title": "1  Small Worlds and Large Worlds",
    "section": "1.3 Practice",
    "text": "1.3 Practice\n\n1.3.1 Easy\n2E1: \\(\\frac{Pr(rain,Monday)}{Pr(Monday)}\\) is read the probability of rain on Monday\n2E2: \\(Pr(Monday|rain)\\) is read the probability that it is Monday, given that it is raining\n2E3: \\(Pr(Monday|rain)\\) is read the probability that it is Monday, given that it is raining\n2E4: The probability of water in 0.7 means that…\n\n\n1.3.2 Medium\n\n1.3.2.1 2M1:\nRecall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\n\nW W W\nW W W L\nL W W L W W W\n\nW W W\n\n# R code 2.4: grid approximation for 20 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=20 ) \n\n# 2) define prior \nprior &lt;- rep( 1 , 20 ) \n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(3 , size=3 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_www &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"W W W\")\n\nW W W L\n\n# R code 2.4: grid approximation for 20 points\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(3 , size=4 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_wwwl &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"W W W L\")\n\nL W W L W W W\n\n# R code 2.4: grid approximation for 20 points\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(5 , size=7 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_lwwlwww &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"L W W L W W W\")\n\n\nlibrary(patchwork)\nfig_grid_www + fig_grid_wwwl + fig_grid_lwwlwww\n\n\n\n\n\n\n1.3.2.2 2M2:\nNow assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n\n# R code 2.4: grid approximation for 20 points\n\n# 1) define grid \np_grid &lt;- seq( from=0 , to=1 , length.out=20 ) \n\n# 2) define prior \nprior &lt;- ifelse(p_grid &lt; .5, 0, 1)\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(3 , size=3 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_www &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"W W W\")\n\nW W W L\n\n# R code 2.4: grid approximation for 20 points\n\nprior &lt;- ifelse(p_grid &lt; .5, 0, 1)\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(3 , size=4 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_wwwl &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"W W W L\")\n\nL W W L W W W\n\n# R code 2.4: grid approximation for 20 points\n\nprior &lt;- ifelse(p_grid &lt; .5, 0, 1)\n\n# 3) compute likelihood at each value in grid \nlikelihood &lt;- dbinom(5 , size=7 , prob=p_grid ) \n# 3 waters (successes) with 3 tosses (size)\n\n# 4) compute product of likelihood and prior \nunstd.posterior &lt;- likelihood * prior \n\n# 5) standardize the posterior, so it sums to 1 \nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n\n# R code 2.4 (my tidyverse version)\n\nfig_grid_lwwlwww &lt;-\n  cbind(p_grid, posterior) |&gt; \n  as_tibble() |&gt; \n  ggplot() +\n  aes(x = p_grid, y = posterior) +\n  geom_line(colour = \"grey\") +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"L W W L W W W\")\n\n\nlibrary(patchwork)\nfig_grid_www + fig_grid_wwwl + fig_grid_lwwlwww\n\n\n\n\n\n\n1.3.2.3 2M3:\nSuppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” \\((Pr(Earth|land))\\), is 0.23.\n\\[\\begin{align}\nP(A|B) &= \\frac{P(B|A)P(A)}{P(B)} \\\\\nP(Earth|land) &= \\frac{P(land|Earth)P(Earth)}{P(land)} \\\\\n0.23 &= \\frac{.3 * .5}{P(land)} \\\\\n0.23 &= \\frac{.3 * .5}{P(land|Earth) * P(land|Mars)} \\\\\n0.23 &= \\frac{.3 * .5}{(0.3 * 0.5) + (1 * 0.5)} \\\\\n0.23 &= \\frac{.3 * .5}{((0.3+1)/2)}\n\\end{align}\\]\n\n0.3*0.5/((0.3+1)/2) #0.2307692\n\n[1] 0.2307692\n\n#prob(earth|land) = (prob(land/earth)*prob(earth))/prob(land)\n\n2M4:\n2M5:\n2M6:\n2M7:\n\n\n\n1.3.3 Hard"
  },
  {
    "objectID": "notes/03-sampling.html#thinking-in-probabilities-vs.-frequencies",
    "href": "notes/03-sampling.html#thinking-in-probabilities-vs.-frequencies",
    "title": "2  Sampling the Imaginary",
    "section": "2.1 Thinking in probabilities vs. frequencies",
    "text": "2.1 Thinking in probabilities vs. frequencies\n\nlet’s say there’s a blood test that detects vampirism 95% of the time\n\nso Pr(positive test result|vampire) = 0.95\n\nthere is also an error rate, in that it makes falsep ositive 1% of the time\n\nso Pr(positive test result|mortal) = 0.01\n\nalso, vampires are rare, comprising of only 0.1% of the population\n\nPr(vampire) = 0.001\n\nusing Bayes’ theorem to find the probability of correctly identifying a vampire with this test (Pr(vampire|position)), we use Equation \\(\\ref{eq-vampire-positive}\\)\n\n\\[\n\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}} \\label{eq-vampire-positive}\n\\]\n\nwe can compute this using these known probabilities\n\n\n\n\n\nTable 2.1: Probabilties\n\n\npr_positive_vampire\n0.950\n\n\npr_positive_mortal\n0.010\n\n\npr_vampire\n0.001\n\n\npr_positive\n0.011\n\n\npr_vampire_positive\n0.087\n\n\n\n\n\n\n\n\n\nor to look at observed frequencies\n\n\n\n\n\nTable 2.2: Frequencies\n\n\npr_vampire\n0.001\n\n\npr_positive_vampire\n0.950\n\n\npr_positive_mortal\n0.001\n\n\npr_positive\n1094.000\n\n\npr_vampire_positive\n0.087\n\n\n\n\n\n\n\n\n\nMcElreath points out that most people find it more intuitive to think about counts, rather than p;ugging probabilities into the right place in Bayes’ theorem, which is often called the frequency format or natural frequencies\n\n\nWorking with samples transforms a problem in calculus into a problem in data summary, into a frequency format problem. - p. 51"
  },
  {
    "objectID": "notes/03-sampling.html#sampling-from-a-grid-like-approximate-posterior",
    "href": "notes/03-sampling.html#sampling-from-a-grid-like-approximate-posterior",
    "title": "2  Sampling the Imaginary",
    "section": "2.2 Sampling from a grid-like approximate posterior",
    "text": "2.2 Sampling from a grid-like approximate posterior"
  },
  {
    "objectID": "notes/03-sampling.html#sapling-to-summarise",
    "href": "notes/03-sampling.html#sapling-to-summarise",
    "title": "2  Sampling the Imaginary",
    "section": "2.3 Sapling to summarise",
    "text": "2.3 Sapling to summarise"
  },
  {
    "objectID": "notes/03-sampling.html#sampling-to-simulate-prediction",
    "href": "notes/03-sampling.html#sampling-to-simulate-prediction",
    "title": "2  Sampling the Imaginary",
    "section": "2.4 Sampling to simulate prediction",
    "text": "2.4 Sampling to simulate prediction"
  },
  {
    "objectID": "notes/03-sampling.html#summary",
    "href": "notes/03-sampling.html#summary",
    "title": "2  Sampling the Imaginary",
    "section": "2.5 Summary",
    "text": "2.5 Summary"
  },
  {
    "objectID": "notes/03-sampling.html#terms-and-concepts",
    "href": "notes/03-sampling.html#terms-and-concepts",
    "title": "2  Sampling the Imaginary",
    "section": "2.6 Terms and concepts",
    "text": "2.6 Terms and concepts\n\n\n\n\nTable 2.3: Useful terms and functions\n\n\nterm/function\ndefinition\n\n\n\n\n`rbinom(n, size, prob)`\n\n\n\n`PI(distribution, prob)`\nrethinking package: compute percentile compatiability interval from posterior\n\n\n`HPDI(distribution, prob)`\nproduces interval that best representats the parameter values most consistent with the data"
  },
  {
    "objectID": "notes/03-sampling.html#practice",
    "href": "notes/03-sampling.html#practice",
    "title": "2  Sampling the Imaginary",
    "section": "2.7 Practice",
    "text": "2.7 Practice\n\n2.7.1 Easy\n\nThe Easy problems use the samples from the posterior distribution for the globe tossing example. This code will give you a specific set of samples, so that you can check your answers exactly…Use the values in samples to answer the questions that follow. - p. 68\n\n\n# R code 3.27\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) \nprior &lt;- rep( 1 , 1000 ) \nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \nposterior &lt;- likelihood * prior \nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100) \nsamples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\n\n\nhist(samples)\n\n\n\n\nFigure 2.1: Histogram of samples\n\n\n\n\n\n2.7.1.1 Easy 1\n\nHow much posterior probability lies below p = 0.2?\n\n\nlength(samples[samples &lt; 0.2])/length(samples)\n\n[1] 0.0004\n\n\n\n# or\nsum(samples &lt; .2)/length(samples)\n\n[1] 0.0004\n\n\n\n\n2.7.1.2 Easy 2\n\nHow much posterior probability lies above p = 0.8?\n\n\nsum(samples &gt; .8)/length(samples)\n\n[1] 0.1116\n\n\n\n\n2.7.1.3 Easy 3\n\nHow much posterior probability lies between p = 0.2 and p = 0.8?\n\n\nsum(samples &lt; 0.8 & samples &gt; 0.2)/length(samples)\n\n[1] 0.888\n\n\n\n\n2.7.1.4 Easy 4\n\n20% of the posterior probability lies below which value of p?\n\n\nquantile(samples, .2)\n\n      20% \n0.5185185 \n\n\n\n\n2.7.1.5 Easy 5\n\n20% of the posterior probability lies above which value of p?\n\n\nquantile(samples, .8)\n\n      80% \n0.7557558 \n\n\n\n\n2.7.1.6 Easy 6\n\nWhich values of p contain the narrowest interval equal to 66% of the posterior probability?\n\nI used the rethinking::PI() function, which is wrong:\n\nPI(samples, prob = .66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\nShould’ve used the rethinking::HDPI function, because we wanted the highest posterior density interval.\n\nHPDI(samples, prob = .66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\n\npacman::p_load(tidybayes)\ntidybayes::mode_hdi(samples, .width = .66)\n\n          y      ymin      ymax .width .point .interval\n1 0.6559935 0.5205205 0.7877878   0.66   mode       hdi\n\n\n\n\n2.7.1.7 Easy 7\n\nWhich values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?\n\nI used the rethinking::HDPI() function, which is wrong:\n\nHPDI(samples, prob = .66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\nI should’ve used the rethinking::PI() function, because e wanted the conventional percentile interval.\n\nPI(samples, prob = .66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\n\n\n\n2.7.2 Medium\n\n2.7.2.1 Medium 1\n\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\n\n# R code 3.27\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) \nprior &lt;- rep( 1 , 1000 ) \nlikelihood &lt;- dbinom( 8 , size=15 , prob=p_grid ) \nposterior &lt;- likelihood * prior \nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100) \nsamples_m &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\n\n\nhist(samples_m)\n\n\n\n\nFigure 2.2: Histogram of samples\n\n\n\n\nCorrect, but McElreath uses this plot:\n\nplot(posterior ~ p_grid, type = \"l\")\n\n\n\n\n\n\n2.7.2.2 Medium 2\n\n3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.\n\n10000 samples is the same thing, no? 1e4 in scientific notation = 10000. I’ll re-run it, but since we’ve set our seed it shouldn’t be any different.\n\nsamples_10g &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\n\nAnd caldulate the 90% HPDI for p:\n\nHPDI(samples_m, prob = .9)\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\nAfter checking teh answers, I was right, it is the same as above, it’s just that question M1 didn’t also ask me to draw the random samples.\n\n\n2.7.2.3 Medium 3"
  },
  {
    "objectID": "notes/04-geometric_models.html#resources",
    "href": "notes/04-geometric_models.html#resources",
    "title": "3  Geocentric Models",
    "section": "Resources",
    "text": "Resources\n\nYouTube lecture (2023): Geocentric Models\nSolomon Kurz: Linear models"
  },
  {
    "objectID": "notes/04-geometric_models.html#set-up",
    "href": "notes/04-geometric_models.html#set-up",
    "title": "3  Geocentric Models",
    "section": "Set-up",
    "text": "Set-up\n\npacman::p_load(\n  rethinking,\n  tidyverse,\n  janitor,\n  knitr,\n  kableExtra\n)\n\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "notes/04-geometric_models.html#permission-to-be-confused",
    "href": "notes/04-geometric_models.html#permission-to-be-confused",
    "title": "3  Geocentric Models",
    "section": "Permission to be confused",
    "text": "Permission to be confused\nFrom ~ 20 minute mark from the YouTue video\n\nyou don’t need to understand everything all at once\n\nmath, concepts, code, installation problems etc.\n\nit’s important to keep some flow\n\nkeep moving forward despite the resistance of confusion/struggle\nthings shouldn’t be too hard, because then you stop moving\nif it’s too easy, you don’t feel any resistance\nfeeling confused means you’re paying attention :)"
  },
  {
    "objectID": "notes/04-geometric_models.html#the-geocentric-model",
    "href": "notes/04-geometric_models.html#the-geocentric-model",
    "title": "3  Geocentric Models",
    "section": "The Geocentric Model",
    "text": "The Geocentric Model\n\nplanets have been observed to have a zig-zag orbit from the perspective of Earth\nGeocentric model/Telomeric model\n\nused to predict the pattern of planets orbit, with the prior of Earth at the centre of the solar system\n\nthe geocentric model was very accurate\n\nhowever, the orbits of the planets are elliptical and orbit around the sun, not the earth\n\nso models built on inaccurate assumptions can still be very accurate in their predictions\nthe orbits of the planets are embedded witin each other (based on their proximity to the sun), so there is a point when two planets’ orbits are closer to each other, i.e., when their orbits have them closer together\n\nwhich is how the perception of Mars’ zig-zag orbit (and other planets, but it’s more stricking for Mars)\n\nin 1809: Bayesian argument for normal error and least-squares estimation from Gauss\nGeocentric: describes association, makes predictions, but mechanisticalistically wrong\n\nthe problem is not the model itself, rather some external causal model we project onto the model\ngeocentric models are still extremely useful, what’s wrong is believing they’re true reflections of e.g., the solar system\nthere’s also nothing wrong with linear regression itself, as long as we don’t believe they’re an accurate mechanistic model of the system we’re studying\n\nGaussian: abstracts from generative error model, replaces with normal distribution, mechanistically silent\n\nusually a pretty good approximation of associations\nwe will be using gaussian error models\n\nuseful when handled with care\n\nmany special cases: ANOVA, ANCOVA, t-test, etc."
  },
  {
    "objectID": "notes/04-geometric_models.html#why-normal-distributions-are-normal",
    "href": "notes/04-geometric_models.html#why-normal-distributions-are-normal",
    "title": "3  Geocentric Models",
    "section": "3.1 Why normal distributions are normal",
    "text": "3.1 Why normal distributions are normal\n\n3.1.1 Normal by multiplication\n\n\n3.1.2 Normal by log-multiplication\n\n\n3.1.3 Using Gaussian distributions"
  },
  {
    "objectID": "notes/04-geometric_models.html#a-language-for-describing-models",
    "href": "notes/04-geometric_models.html#a-language-for-describing-models",
    "title": "3  Geocentric Models",
    "section": "3.2 A language for describing models",
    "text": "3.2 A language for describing models"
  },
  {
    "objectID": "notes/04-geometric_models.html#a-gaussian-model-of-height",
    "href": "notes/04-geometric_models.html#a-gaussian-model-of-height",
    "title": "3  Geocentric Models",
    "section": "3.3 A Gaussian model of height",
    "text": "3.3 A Gaussian model of height\n\n# load Howell1 data from rethinking package\ndata(Howell1) \ndf_weight &lt;- Howell1\n\n\nhead(df_weight)\n\n   height   weight age male\n1 151.765 47.82561  63    1\n2 139.700 36.48581  63    0\n3 136.525 31.86484  65    0\n4 156.845 53.04191  41    1\n5 145.415 41.27687  51    0\n6 163.830 62.99259  35    1\n\n\n\nsummary(df_weight)\n\n     height           weight            age             male       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.00   Min.   :0.0000  \n 1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00   1st Qu.:0.0000  \n Median :148.59   Median :40.058   Median :27.00   Median :0.0000  \n Mean   :138.26   Mean   :35.611   Mean   :29.34   Mean   :0.4724  \n 3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00   3rd Qu.:1.0000  \n Max.   :179.07   Max.   :62.993   Max.   :88.00   Max.   :1.0000  \n\n\n\nage ranges from babies to seniors\nsex as binary\nheight in cm\nweight in kg\n\nprecis() function from rethinking package:\n\nprecis(df_weight) |&gt; \n  as_tibble() |&gt; \n  kable(digits = 1) |&gt; \n  kable_styling()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n138.3\n\n\n35.6\n\n\n29.3\n\n\n0.5\n\n\n\n\n\n\nx\n\n\n\n\n27.6\n\n\n14.7\n\n\n20.7\n\n\n0.5\n\n\n\n\n\n\nx\n\n\n\n\n81.1\n\n\n9.4\n\n\n1.0\n\n\n0.0\n\n\n\n\n\n\nx\n\n\n\n\n165.7\n\n\n54.5\n\n\n66.1\n\n\n1.0\n\n\n\n\n\n\nx\n\n\n\n\n▁▁▁▁▁▁▁▂▁▇▇▅▁\n\n\n▁▂▃▂▂▂▂▅▇▇▃▂▁\n\n\n▇▅▅▃▅▂▂▁▁\n\n\n▇▁▁▁▁▁▁▁▁▇\n\n\n\n\n\n\n\n\n\n\nTo define heights as normally distributed, we write \\(\\ref{eq-height-gauss}\\), where \\(i\\) stands for index. \\(\\ref{eq-height-gauss}\\) is saying that all the golem (i.e., our model) knows about each observed height measurement is defined by the same normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\\[\\begin{align}\nh_i \\sim Normal(\\mu, \\sigma) \\label{eq-height-gauss} \\\\\n\\mu \\sim Normal(178, 20) \\label{eq-height-prior-mu} \\\\\n\\sigma \\sim Uniform(0, 50) \\label{eq-height-prior-sigma}\n\\end{align}\\]\n\\(\\ref{eq-height-prior-mu}\\) represents our prior for \\(\\mu\\), where our assumed mean is 178 cm (because Richard McElreath is 178cm tall), with a 95% probability between +/- 40cm (because if the SD is 20cm, 20 x 1.96 is approximately 20 x 2).\n\n# R chunk 4.10\n\n# subset data for adults only\nd2 &lt;- df_weight |&gt; filter(age &gt; 18)\n\nWe can plot our priors:\n\n# R code chunk 4.12\ncurve(dnorm(x, 178, 20), from = 100, to = 250)\n\n\n\n\nOr, with ggplot (from Solomon Kurz):\n\nggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), \n       aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\n  geom_line() +\n  ylab(\"density\")\n\n\n\n\nWe can also view our prior for \\(\\sigma\\), which is a flat (uniform) prior:\n\n# R code chunk 4.13\ncurve(dunif(x, 0, 50), from = 10, to = 60)\n\n\n\n\nAnd with ggplot (from Solomon Kurz):\n\ntibble(x = seq(from = -10, to = 60, by = .1)) %&gt;%\n  \n  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +\n  geom_line() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\nA standard deviation must be positiion, and so we’ve bound it at zero. Picking the upperbound is another question: in our case a sd of 50cm would imply that 95% of individual heights lie within 100cm of the average height (again, because SD * 1.96), which is a very large range.\nNow it’s time for a ~prior predictive~ simulation, for which we need our chosen priors for \\(h, \\mu,\\) and \\(\\sigma\\), which imply a joint prior distribution of individual heights.\nLet’s quickly simiulate heights by sampling from our prior. We can process priors just like posteriors, because every posterior is also potentially a prior for a subsequent analysis.\n\n# R code 4.14\nsample_mu &lt;- rnorm(1e4, 178, 20)\nsample_sigma &lt;- runif(1e4, 0, 50)\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\nAnd sampling from both \\(\\mu\\) and \\(\\sigma\\) priors to get a prior probability of heights:\n\nn &lt;- 1e4\n\nset.seed(4)\ntibble(sample_mu    = rnorm(n, mean = 178,       sd  = 20),\n       sample_sigma = runif(n, min  = 0,         max = 50)) %&gt;% \n  mutate(x = rnorm(n, mean = sample_mu, sd  = sample_sigma)) %&gt;% \n  \n  ggplot(aes(x = x)) +\n  geom_density(fill = \"red\", linewidth = 0, alpha = .25) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(Prior~predictive~distribution~\"for\"~italic(h[i])),\n       x = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n3.3.1 Finding the posterior distribution with quap\n\n# R chunk 2.23\nd3 &lt;- sample( d2$height , size=20 )\n\n\n# R chunk 4.24\nmu.list &lt;- seq( from=150, to=170 , length.out=200 ) \nsigma.list &lt;- seq( from=4 , to=20 , length.out=200 ) \npost2 &lt;- expand.grid( mu=mu.list , sigma=sigma.list ) \npost2$LL &lt;- sapply( 1:nrow(post2) , function(i) sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) )\npost2$prod &lt;- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) \npost2$prob &lt;- exp( post2$prod - max(post2$prod) )\nsample2.rows &lt;- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) \nsample2.mu &lt;- post2$mu[ sample2.rows ] \nsample2.sigma &lt;- post2$sigma[ sample2.rows ]\nplot( sample2.mu , sample2.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab=\"mu\" , ylab=\"sigma\" , pch=16 )\n\n\n\n\n\n# R code 4.26\ndata(Howell1)\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18 , ]\n\nAnd define our model and place it into an alist.\n\n# R code 4.27\nflist &lt;-\n  alist(height ~ dnorm(mu , sigma) , \n        mu ~ dnorm(178 , 20) , \n        sigma ~ dunif(0 , 50)\n        )\n\nFit a model to the data in d2.\n\n# R code 4.28\nm4.1 &lt;- quap(flist, data = d2)\n\nTake a look.\n\n# R code 4.29\nprecis(m4.1)\n\n            mean        sd       5.5%      94.5%\nmu    154.606899 0.4119494 153.948524 155.265273\nsigma   7.730482 0.2913059   7.264919   8.196045\n\n\n\n\n3.3.2 Simulate weights\nFrom the GitHub script 03_howell etc.\n\ndata(Howell1)\nd &lt;- Howell1\n\n# draw data\n\nblank(bty=\"n\")\n\ncol2 &lt;- col.alpha(2,0.8)\nplot( d$height , d$weight , col=col2 , lwd=3 , cex=1.2 , xlab=\"height (cm)\" , ylab=\"weight (kg)\" )\n\n\n\nd &lt;- d[ d$age&gt;=18 , ]\nplot( d$height , d$weight , col=col2 , lwd=3 , cex=1.2 , xlab=\"height (cm)\" , ylab=\"weight (kg)\" )\n\n\n\n\n\n\n3.3.3 Describing models\n(video c. 31:00)\nConventional statistical model notaton:\n\nlist the variables in the model\ndefine each variable as a deterministic or distributional function of the other variables\n\n\nwhere does each variable get its value?\n\n\n# function to simulate weights of individuals from height \nsim_weight &lt;- function(H,b,sd) {\n  U &lt;- rnorm(length(H), 0, sd)\n  W &lt;- b*H + U\n  return(W)\n}\n\n\nthis is a function of unobserved U, which has normal error\nwhere W is a deterministic function of H and U together\nthis can be re-written with standard statistical notation as followed:\n\n\\[\\begin{align}\nW_i &= \\beta H_i + U_i \\label{eq-W} \\\\\nU_i &\\sim Normal(0,\\sigma) \\label{eq-U} \\\\\nH_i &\\sim Uniform(130,170) \\label{eq-H}\n\\end{align}\\]\nEach of these is mirrored in a line of code. Along the left of the equations are the variables, along the right are their definitions. The subscripts indicate individuals (indexing).\n\n\\(=\\) indicates a deterministic relationship\n\\(\\sim\\) indicates a distributional relationship\n\nEquation \\(\\ref{eq-W}\\) is the equation for expected weight. Once we know the values to the right of this equation we can assign values to W. Equation \\(\\ref{eq-U}\\) represents Gaussian error with standard deviation sigma. Equation \\(\\ref{eq-H}\\) defines height as uniformly distributed from 130cm to 170cm. These are all defined in the code chunk above, but written in the opposite order. In the code it’s written in order of execution, but the statistical conventional notation sometimes has arbitrary order, but conventionally is stated in the opposite direction as the code is written because you start with the most general definition (\\(\\ref{eq-W}\\)) which depends on the other variables, and then you see how the variables W depends upon are defined. There’s a hierarchy that makes sense.\n\n\n3.3.4 Estimator\nWe want to estimate how the average weight changes with height, i.e., how the average weight changes with height.\n\\[\\begin{equation}\nE(W_i|H_i) = \\alpha + \\beta H_i\n\\end{equation}\\]\nWhere \\(E(W_i|H_i)\\) is the average weight conditional on height, \\(\\alpha\\) is the intercept and \\(\\beta\\) is our slope. So each height has a different average weight.\n\n\n3.3.5 Posterior distribution\nGood Bayesians estimate a posterior. The equation below is the same as that for when we tossed the globe, but we just have more unknowns now.\n\\[\\begin{equation}\nPr(\\alpha, \\beta, \\sigma|H_i, W_i) = \\frac\n{Pr(W_i|H_i, \\alpha, \\beta, \\sigma)Pr(\\alpha, \\beta, \\sigma)}\n{Z}\n\\end{equation}\\]\n\n\\(Pr(\\alpha, \\beta, \\sigma|H_i, W_i)\\) is the posterior probability of a specific regression line, which is defined by , , and (unobserved variables). H and W are our observed data, we don’t need a posterior distribution for them, they’re observed (the likelihood).\n\\(Pr(W_i|H_i, \\alpha, \\beta, \\sigma)\\): garden of forking data\n\\(Pr(\\alpha, \\beta, \\sigma)\\): our priors\n\\(Z\\): a normalising constant that we don’t need to pay too much attention to, it ensures the lefthand side of the equation is a proper probability\n\nW is normally distributed with mean that is a a linear function of H.\nThese models are typically written as below:\n\\[\\begin{align}\nW_i &= Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &\\sim \\alpha + \\beta H_i\n\\end{align}\\]\n\n\n3.3.6 Grid approximat posterior\nAll posterior distributions begin with a single observations. There’s no minimum number of observations in Bayesian inference.\n\n\n3.3.7 Quardratic approximation\n\nwe’ll approximate the posterior distribution as a multivariate Gaussian distribution. We can do that because posterior distributions are typically multivariate Gaussian distributions if there are enough observations. A tool that does this for us is called quap() in the statrethinking package.\n\nThe Gaussian distribution is quadratic, and in the log-space the Gaussian distribution is a perfect parabola (i.e., quadratic).\nWhen there are no observations, what does the model believe? These are our priors with no likelihood.\n\n# this wasn't working...\nm3.1 &lt;-\n  quap(\n    alist(\n      W ~ dnorm(mu, sigma),\n      mu &lt;- a + b*Hm,\n      a ~ dnorm(0,10),\n      b ~ dunif(0,1),\n      sigma ~ dunif(0, 10)\n    ),\n    data - list(W=W, H=H)\n  )"
  },
  {
    "objectID": "notes/04-geometric_models.html#linear-prediction-adding-a-predictor",
    "href": "notes/04-geometric_models.html#linear-prediction-adding-a-predictor",
    "title": "3  Geocentric Models",
    "section": "3.4 Linear Prediction: Adding a predictor",
    "text": "3.4 Linear Prediction: Adding a predictor\nWe’ll now add a predictor to our model: weight. So we’ll look at how height covaries with weight (our predictor).\nA scatter plot of weight by height (with ggplot):\n\nggplot(data = d2, \n       aes(x = weight, y = height)) +\n  geom_point(shape = 1, size = 2) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\nWe now have:\n\\[\\begin{align}\nh_i &\\sim Normal(\\mu_i, \\sigma) \\hspace{35pt}likelihood \\label{eq-likelihood} \\\\\n\\mu_i &\\sim \\alpha + \\beta(x_i - \\bar{x}) \\hspace{35pt}linear\\;model \\label{eq-linear} \\\\\n\\alpha &\\sim Normal(178, 20) \\hspace{35pt}\\beta \\; prior \\label{eq-alpha}\\\\\n\\beta &\\sim Normal(178, 20) \\hspace{35pt}\\mu \\; prior \\label{eq-mu}\\\\\n\\sigma &\\sim Uniform(0, 50) \\hspace{35pt}\\sigma \\; prior \\label{eq-sigma}\n\\end{align}\\]\nwhere the average of \\(x\\) values is \\(\\bar{x}\\). Our predictor variablle is \\(x\\), which is a list of measures of the same length as \\(h\\). We now define \\(\\mu\\) as a function of the values in \\(x\\) in order to include our predictor, weight, in the model. \\(\\alpha\\) is our intercept, \\(\\beta\\) is our slope."
  },
  {
    "objectID": "notes/04-geometric_models.html#polynomial-regression",
    "href": "notes/04-geometric_models.html#polynomial-regression",
    "title": "3  Geocentric Models",
    "section": "3.5 Polynomial regression",
    "text": "3.5 Polynomial regression"
  },
  {
    "objectID": "notes/04-geometric_models.html#solomon-kurz",
    "href": "notes/04-geometric_models.html#solomon-kurz",
    "title": "3  Geocentric Models",
    "section": "3.6 Solomon Kurz",
    "text": "3.6 Solomon Kurz\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.20.3). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:rethinking':\n\n    LOO, stancode, WAIC\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\n\n3.6.1 Fitting a model with brm()\nInstead of statrethinking::map()\n\nd2 &lt;- \n  d %&gt;%\n  filter(age &gt;= 18)\n\n\nb4.1 &lt;- \n  brm(data = d2, \n      family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = here::here(\"fits/b04.01\"))\n\n\nplot(b4.1) \n\n\n\n\n\nprint(b4.1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.41   153.80   155.43 1.00     2887     2156\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.30     7.22     8.38 1.00     3322     2196\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOr, if we want to use the 89% intervals used in the textbook:\n\nsummary(b4.1, prob = .89)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.41   153.95   155.24 1.00     2887     2156\n\nFamily Specific Parameters: \n      Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.30     7.31     8.26 1.00     3322     2196\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow if we want very narrow priors (sd of 0.1!):\n\nb4.2 &lt;- \n  brm(data = d2, family = gaussian,\n      height ~ 1,\n      prior = c(prior(normal(178, 0.1), class = Intercept),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = here::here(\"fits/b04.02\"))\n\n\nplot(b4.2)\n\n\n\n\nWe see the chains actually look pretty good. Let’s look at the fixed effects to easily compare the estimates of the two models:\n\nrbind(\n  summary(b4.1)$fixed |&gt; \n  as_tibble() |&gt; \n  mutate(model = \"b4.1\"),\nsummary(b4.2)$fixed |&gt; \n  as_tibble() |&gt; \n  mutate(model = \"b4.2\")\n)\n\n# A tibble: 2 × 8\n  Estimate Est.Error `l-95% CI` `u-95% CI`  Rhat Bulk_ESS Tail_ESS model\n     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1     155.     0.411       154.       155.  1.00    2887.    2156. b4.1 \n2     178.     0.101       178.       178.  1.00    3066.    2437. b4.2 \n\n\n\n\n3.6.2 Sampleing from a brm() fit\nInstead of using the vcov() function from the statrethinking package (brms has a vcov() function but it only gives the first element in the matrix), we can get it after putting the HMC chains in a data frame (which we do with as_draws_df()):\n\npost &lt;- as_draws_df(b4.1)\n\nhead(post)\n\n# A draws_df: 6 iterations, 1 chains, and 4 variables\n  b_Intercept sigma lprior  lp__\n1         155   7.5   -8.5 -1227\n2         155   7.0   -8.5 -1230\n3         154   7.6   -8.5 -1226\n4         154   8.0   -8.5 -1226\n5         155   7.6   -8.5 -1226\n6         155   7.4   -8.5 -1227\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\nselect(post, b_Intercept:sigma) %&gt;% \n  cov()\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n             b_Intercept        sigma\nb_Intercept  0.168810743 -0.000740092\nsigma       -0.000740092  0.087416426"
  },
  {
    "objectID": "notes/04-geometric_models.html#practice",
    "href": "notes/04-geometric_models.html#practice",
    "title": "3  Geocentric Models",
    "section": "3.7 Practice",
    "text": "3.7 Practice\n\nE1\n\nIn the model definition below, which line is the likelihood? \\[\\begin{align}\ny_i &\\sim Normal(\\mu,\\sigma) \\label{ex-e1a} \\\\\n\\mu &\\sim Normal(0,10) \\label{ex-e1b} \\\\\n\\sigma &\\sim Exponential(1) \\label{ex-e1c}\n\\end{align}\\]\n\n\\(\\ref{ex-e1a}\\) is the likelihood (\\(\\ref{ex-e1b}\\) is the prior for \\(\\mu\\) and \\(\\ref{ex-e1c}\\) for \\(\\sigma\\))\n\n\nE2\n\nIn the model definition just above, how many parameters are in the posterior distribution?\n\nThere are two parameters in the posterior distribution: \\(\\mu\\) and \\(\\sigma\\)\n\n\nE3\n\nUsing the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.\n\nBayes’ theorem is:\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\end{equation}\\]\nWhere \\(A\\) is our priors and \\(B\\) is the posterior, i.e., observed data (\\(y\\)) I’m guessing? And so…\n\\[\\begin{equation}\nP(\\mu,\\sigma|y) = \\frac{P(y|\\mu,\\sigma)P(\\mu)P(\\sigma)}{P(y)}\n\\end{equation}\\]\nOkay I really don’t know…I guess we add in the model parameter definitions?\n\n\nE4\n\nIn the model definition below, which line is the linear model? \\[\\begin{align}\ny_i &\\sim Normal(\\mu,\\sigma) \\label{ex-e4a} \\\\\n\\mu &= \\alpha + \\beta x_i \\label{ex-e4b} \\\\\n\\alpha &\\sim Normal(0,10) \\label{ex-e4c} \\\\\n\\beta &\\sim Normal(0,1) \\label{ex-e4d} \\\\\n\\sigma &\\sim Exponential(2) \\label{ex-e4e}\n\\end{align}\\]\n\n\\(\\ref{ex-e4b}\\) is the linear model. is the likelihood, the prior for \\(\\alpha\\) (intercept), the prior for \\(\\beta\\) (slope), and the prior for sigma (sd).\n\n\nE5\n\nIn the model definition just above, how many parameters are in the posterior distribution?\n\nThe posterior distribution has the parameters:\n\n\\(\\sigma\\): standard deviation\n\\(\\alpha\\): intercept\n\\(\\beta\\): slope\n\n\n\nM1\n\nFor the model definition below, simulate observed y values from the prior (not the posterior). \\[\\begin{align}\ny_i &\\sim Normal(\\mu,\\sigma) \\label{ex-m1a} \\\\\n\\mu &\\sim Normal(0,10) \\label{ex-m1b} \\\\\n\\sigma &\\sim Exponential(1) \\label{ex-m1c}\n\\end{align}\\]\n\n\n# same as R chunk 4.14\nsample_mu &lt;- rnorm(1e4, 0, 10)\nsample_sigma &lt;- rexp(1e4, 1)\n# simulate prior predictive values\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\nAnd our prior predictive with ggplot:\n\nn &lt;- 1e4\n\nset.seed(4)\ntibble(sample_mu    = rnorm(n, mean = 0,       sd  = 10),\n       sample_sigma = rexp(n, 1)) %&gt;% \n  mutate(x = rnorm(n, mean = sample_mu, sd  = sample_sigma)) %&gt;% \n  \n  ggplot(aes(x = x)) +\n  geom_density(fill = \"red\", linewidth = 0, alpha = .25) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(Prior~predictive~distribution~\"for\"~italic(h[i])),\n       x = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n3.7.1 Sampling from the posterior\n\n# R chunk 4.15\nsample_mu &lt;- rnorm( 1e4 , 178 , 100 ) \nprior_h &lt;- rnorm( 1e4 , sample_mu , sample_sigma ) \ndens( prior_h )\n\n\n\n\n\n# R chunk 4.16\nmu.list &lt;- seq( from=150, to=160 , length.out=100 ) \nsigma.list &lt;- seq( from=7 , to=9 , length.out=100 ) \npost &lt;- expand.grid( mu=mu.list , sigma=sigma.list ) \npost$LL &lt;- sapply( 1:nrow(post) , function(i) \n  sum( dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) \n\npost$prod &lt;- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) + dunif( post$sigma , 0 , 50 , TRUE ) \npost$prob &lt;- exp( post$prod - max(post$prod) )\n\n\n# R chunk 4.17\nsample.rows &lt;- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob ) \nsample.mu &lt;- post$mu[ sample.rows ] \nsample.sigma &lt;- post$sigma[ sample.rows ]\n\n\n# R chunk 4.21\ndens(sample_mu)\n\n\n\n\n\n# R chunk 4.21\ndens(sample_sigma)\n\n\n\n\n\n\nM2\n\nTranslate the model just above into a quap formula.\n\nDo this as in R chunk 4.26:\n\nflist &lt;-\n  alist(height ~ dnorm(mu , sigma) , \n        mu ~ dnorm(0 , 10) , \n        sigma ~ dexp(0 , 1)\n        )"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kurz, A. Solomon. 2023. Statistical\nRethinking with brms, ggplot2, and the tidyverse.\nVersion 1.3.0. https://bookdown.org/content/3890/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. Second edition. Chapman &\nHall/CRC Texts in Statistical Science Series.\nBoca Raton: CRC Press."
  }
]